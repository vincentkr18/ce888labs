{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to machine learning with scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the [scikit-learn documentation](http://scikit-learn.org/stable/tutorial/basic/tutorial.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the 'breast cancer' dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn comes with a few standard datasets, for instance the iris and digits datasets for classification and the Boston house prices dataset for regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19.2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "cancer = datasets.load_breast_cancer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A dataset is a dictionary-like object that holds all the data and some metadata about the data. This data is stored in the `.data` member, which is a `n_samples` by `n_features` array. In the case of supervised problem, one or more response variables are stored in the `.target` member.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'target_names', 'DESCR', 'feature_names'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cancer.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['malignant', 'benign'], dtype='<U9')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cancer.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 30 features in this dataset\n",
      "The features are: ['mean radius' 'mean texture' 'mean perimeter' 'mean area'\n",
      " 'mean smoothness' 'mean compactness' 'mean concavity'\n",
      " 'mean concave points' 'mean symmetry' 'mean fractal dimension'\n",
      " 'radius error' 'texture error' 'perimeter error' 'area error'\n",
      " 'smoothness error' 'compactness error' 'concavity error'\n",
      " 'concave points error' 'symmetry error' 'fractal dimension error'\n",
      " 'worst radius' 'worst texture' 'worst perimeter' 'worst area'\n",
      " 'worst smoothness' 'worst compactness' 'worst concavity'\n",
      " 'worst concave points' 'worst symmetry' 'worst fractal dimension']\n"
     ]
    }
   ],
   "source": [
    "n_features = len(cancer.feature_names)\n",
    "print(\"There are %d features in this dataset\" % n_features)\n",
    "print(\"The features are:\", cancer.feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For instance, in the case of the breast cancer dataset, cancer.data gives access to the features that can be used to classify the samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(569, 30)\n",
      "[[1.799e+01 1.038e+01 1.228e+02 ... 2.654e-01 4.601e-01 1.189e-01]\n",
      " [2.057e+01 1.777e+01 1.329e+02 ... 1.860e-01 2.750e-01 8.902e-02]\n",
      " [1.969e+01 2.125e+01 1.300e+02 ... 2.430e-01 3.613e-01 8.758e-02]\n",
      " ...\n",
      " [1.660e+01 2.808e+01 1.083e+02 ... 1.418e-01 2.218e-01 7.820e-02]\n",
      " [2.060e+01 2.933e+01 1.401e+02 ... 2.650e-01 4.087e-01 1.240e-01]\n",
      " [7.760e+00 2.454e+01 4.792e+01 ... 0.000e+00 2.871e-01 7.039e-02]]\n"
     ]
    }
   ],
   "source": [
    "print(cancer.data.shape)\n",
    "print(cancer.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and `cancer.target` gives the ground truth for the dataset, that is whether the tumor is benign or not:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(569,)\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 1 0 1 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1 0 0 1 1 1 1 0 1 0 0\n",
      " 1 0 1 0 0 1 1 1 0 0 1 0 0 0 1 1 1 0 1 1 0 0 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1\n",
      " 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1 0 0 1 0 1 0 0 1 0 0 1 1 0 1 1 0 1 1 1 1 0 1\n",
      " 1 1 1 1 1 1 1 1 0 1 1 1 1 0 0 1 0 1 1 0 0 1 1 0 0 1 1 1 1 0 1 1 0 0 0 1 0\n",
      " 1 0 1 1 1 0 1 1 0 0 1 0 0 0 0 1 0 0 0 1 0 1 0 1 1 0 1 0 0 0 0 1 1 0 0 1 1\n",
      " 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 1 0 1 1 1 1 0 1 1 1 1 1 0 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 1 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 0 1 1 1 1 0 0 0 1 1\n",
      " 1 1 0 1 0 1 0 1 1 1 0 1 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0\n",
      " 0 1 0 0 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 0 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1\n",
      " 1 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 0 1 1 1 1 1 0 1 1\n",
      " 0 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1\n",
      " 1 1 1 1 1 1 0 1 0 1 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 0 1 1 0 1 0 1 0 0\n",
      " 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 0 0 0 0 0 0 1]\n",
      "['malignant' 'benign']\n"
     ]
    }
   ],
   "source": [
    "print(cancer.target.shape)\n",
    "print(cancer.target)\n",
    "print(cancer.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualising the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is your chance to have a look at the data. Try some of the things from the seaborn/pandas lab session. What's easier for you, to work with this sort of dataset or with a pandas dataframe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(cancer.data[:, 0], cancer.data[:, 1])\n",
    "plt.xlabel(cancer.feature_names[0])\n",
    "plt.ylabel(cancer.feature_names[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert your own code visualization/analysis here.\n",
    "# Try to come up with a method that you can use to determine whether your data requires any sort of standarisation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning and predicting\n",
    "In the case of the breast cancer dataset, the task is to predict, given some features, whether the tumor is benign or malign. We are given samples of each case, and with these samples we fit an estimator to be able to predict the classes to which unseen samples belong.\n",
    "\n",
    "In scikit-learn, an estimator for classification is a Python object that implements the methods `fit(X, y)` and `predict(T)`.\n",
    "\n",
    "An example of an estimator is the class `sklearn.svm.SVC` that implements support vector classification. The constructor of an estimator takes as arguments the parameters of the model, but for the time being, we will consider the estimator as a black box:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "clf = svm.SVC(gamma=0.0001, C=100.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We call our estimator instance `clf`, as it is a classifier. **It now must be fitted to the model, that is, it must learn from the data**. This is done by passing our training set to the `fit` method. As a training set, let us use all the examples of our dataset except for the last one. We select this training set with the `[:-1]` Python syntax, which produces a new array that contains **all but the last entry** of `cancer.data`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=100.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma=0.0001, kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(cancer.data[:-1], cancer.target[:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can predict new values, in particular, we can ask to the classifier whether the tumor from the last example is benign or not. **Remember that this patient was NOT used to train the classifier**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The actual value is [1]\n"
     ]
    }
   ],
   "source": [
    "print(\"The predicted value is\", clf.predict(cancer.data[-1:]))##Insert code here\n",
    "#cancer.data[-1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also check what the real label for this patient was:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The actual value is [1]\n"
     ]
    }
   ],
   "source": [
    "#Insert code here\n",
    "print(\"The actual value is\", cancer.target[-1:].astype(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you print the actual raw values of the 30 features for this patient?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.990</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.71190</td>\n",
       "      <td>0.26540</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.570</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.24160</td>\n",
       "      <td>0.18600</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.690</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.45040</td>\n",
       "      <td>0.24300</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.420</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.68690</td>\n",
       "      <td>0.25750</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.290</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.40000</td>\n",
       "      <td>0.16250</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12.450</td>\n",
       "      <td>15.70</td>\n",
       "      <td>82.57</td>\n",
       "      <td>477.1</td>\n",
       "      <td>0.12780</td>\n",
       "      <td>0.17000</td>\n",
       "      <td>0.15780</td>\n",
       "      <td>0.08089</td>\n",
       "      <td>0.2087</td>\n",
       "      <td>0.07613</td>\n",
       "      <td>...</td>\n",
       "      <td>23.75</td>\n",
       "      <td>103.40</td>\n",
       "      <td>741.6</td>\n",
       "      <td>0.1791</td>\n",
       "      <td>0.5249</td>\n",
       "      <td>0.53550</td>\n",
       "      <td>0.17410</td>\n",
       "      <td>0.3985</td>\n",
       "      <td>0.12440</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>18.250</td>\n",
       "      <td>19.98</td>\n",
       "      <td>119.60</td>\n",
       "      <td>1040.0</td>\n",
       "      <td>0.09463</td>\n",
       "      <td>0.10900</td>\n",
       "      <td>0.11270</td>\n",
       "      <td>0.07400</td>\n",
       "      <td>0.1794</td>\n",
       "      <td>0.05742</td>\n",
       "      <td>...</td>\n",
       "      <td>27.66</td>\n",
       "      <td>153.20</td>\n",
       "      <td>1606.0</td>\n",
       "      <td>0.1442</td>\n",
       "      <td>0.2576</td>\n",
       "      <td>0.37840</td>\n",
       "      <td>0.19320</td>\n",
       "      <td>0.3063</td>\n",
       "      <td>0.08368</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>13.710</td>\n",
       "      <td>20.83</td>\n",
       "      <td>90.20</td>\n",
       "      <td>577.9</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>0.16450</td>\n",
       "      <td>0.09366</td>\n",
       "      <td>0.05985</td>\n",
       "      <td>0.2196</td>\n",
       "      <td>0.07451</td>\n",
       "      <td>...</td>\n",
       "      <td>28.14</td>\n",
       "      <td>110.60</td>\n",
       "      <td>897.0</td>\n",
       "      <td>0.1654</td>\n",
       "      <td>0.3682</td>\n",
       "      <td>0.26780</td>\n",
       "      <td>0.15560</td>\n",
       "      <td>0.3196</td>\n",
       "      <td>0.11510</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>13.000</td>\n",
       "      <td>21.82</td>\n",
       "      <td>87.50</td>\n",
       "      <td>519.8</td>\n",
       "      <td>0.12730</td>\n",
       "      <td>0.19320</td>\n",
       "      <td>0.18590</td>\n",
       "      <td>0.09353</td>\n",
       "      <td>0.2350</td>\n",
       "      <td>0.07389</td>\n",
       "      <td>...</td>\n",
       "      <td>30.73</td>\n",
       "      <td>106.20</td>\n",
       "      <td>739.3</td>\n",
       "      <td>0.1703</td>\n",
       "      <td>0.5401</td>\n",
       "      <td>0.53900</td>\n",
       "      <td>0.20600</td>\n",
       "      <td>0.4378</td>\n",
       "      <td>0.10720</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>12.460</td>\n",
       "      <td>24.04</td>\n",
       "      <td>83.97</td>\n",
       "      <td>475.9</td>\n",
       "      <td>0.11860</td>\n",
       "      <td>0.23960</td>\n",
       "      <td>0.22730</td>\n",
       "      <td>0.08543</td>\n",
       "      <td>0.2030</td>\n",
       "      <td>0.08243</td>\n",
       "      <td>...</td>\n",
       "      <td>40.68</td>\n",
       "      <td>97.65</td>\n",
       "      <td>711.4</td>\n",
       "      <td>0.1853</td>\n",
       "      <td>1.0580</td>\n",
       "      <td>1.10500</td>\n",
       "      <td>0.22100</td>\n",
       "      <td>0.4366</td>\n",
       "      <td>0.20750</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>16.020</td>\n",
       "      <td>23.24</td>\n",
       "      <td>102.70</td>\n",
       "      <td>797.8</td>\n",
       "      <td>0.08206</td>\n",
       "      <td>0.06669</td>\n",
       "      <td>0.03299</td>\n",
       "      <td>0.03323</td>\n",
       "      <td>0.1528</td>\n",
       "      <td>0.05697</td>\n",
       "      <td>...</td>\n",
       "      <td>33.88</td>\n",
       "      <td>123.80</td>\n",
       "      <td>1150.0</td>\n",
       "      <td>0.1181</td>\n",
       "      <td>0.1551</td>\n",
       "      <td>0.14590</td>\n",
       "      <td>0.09975</td>\n",
       "      <td>0.2948</td>\n",
       "      <td>0.08452</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>15.780</td>\n",
       "      <td>17.89</td>\n",
       "      <td>103.60</td>\n",
       "      <td>781.0</td>\n",
       "      <td>0.09710</td>\n",
       "      <td>0.12920</td>\n",
       "      <td>0.09954</td>\n",
       "      <td>0.06606</td>\n",
       "      <td>0.1842</td>\n",
       "      <td>0.06082</td>\n",
       "      <td>...</td>\n",
       "      <td>27.28</td>\n",
       "      <td>136.50</td>\n",
       "      <td>1299.0</td>\n",
       "      <td>0.1396</td>\n",
       "      <td>0.5609</td>\n",
       "      <td>0.39650</td>\n",
       "      <td>0.18100</td>\n",
       "      <td>0.3792</td>\n",
       "      <td>0.10480</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>19.170</td>\n",
       "      <td>24.80</td>\n",
       "      <td>132.40</td>\n",
       "      <td>1123.0</td>\n",
       "      <td>0.09740</td>\n",
       "      <td>0.24580</td>\n",
       "      <td>0.20650</td>\n",
       "      <td>0.11180</td>\n",
       "      <td>0.2397</td>\n",
       "      <td>0.07800</td>\n",
       "      <td>...</td>\n",
       "      <td>29.94</td>\n",
       "      <td>151.70</td>\n",
       "      <td>1332.0</td>\n",
       "      <td>0.1037</td>\n",
       "      <td>0.3903</td>\n",
       "      <td>0.36390</td>\n",
       "      <td>0.17670</td>\n",
       "      <td>0.3176</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>15.850</td>\n",
       "      <td>23.95</td>\n",
       "      <td>103.70</td>\n",
       "      <td>782.7</td>\n",
       "      <td>0.08401</td>\n",
       "      <td>0.10020</td>\n",
       "      <td>0.09938</td>\n",
       "      <td>0.05364</td>\n",
       "      <td>0.1847</td>\n",
       "      <td>0.05338</td>\n",
       "      <td>...</td>\n",
       "      <td>27.66</td>\n",
       "      <td>112.00</td>\n",
       "      <td>876.5</td>\n",
       "      <td>0.1131</td>\n",
       "      <td>0.1924</td>\n",
       "      <td>0.23220</td>\n",
       "      <td>0.11190</td>\n",
       "      <td>0.2809</td>\n",
       "      <td>0.06287</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>13.730</td>\n",
       "      <td>22.61</td>\n",
       "      <td>93.60</td>\n",
       "      <td>578.3</td>\n",
       "      <td>0.11310</td>\n",
       "      <td>0.22930</td>\n",
       "      <td>0.21280</td>\n",
       "      <td>0.08025</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.07682</td>\n",
       "      <td>...</td>\n",
       "      <td>32.01</td>\n",
       "      <td>108.80</td>\n",
       "      <td>697.7</td>\n",
       "      <td>0.1651</td>\n",
       "      <td>0.7725</td>\n",
       "      <td>0.69430</td>\n",
       "      <td>0.22080</td>\n",
       "      <td>0.3596</td>\n",
       "      <td>0.14310</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>14.540</td>\n",
       "      <td>27.54</td>\n",
       "      <td>96.73</td>\n",
       "      <td>658.8</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.15950</td>\n",
       "      <td>0.16390</td>\n",
       "      <td>0.07364</td>\n",
       "      <td>0.2303</td>\n",
       "      <td>0.07077</td>\n",
       "      <td>...</td>\n",
       "      <td>37.13</td>\n",
       "      <td>124.10</td>\n",
       "      <td>943.2</td>\n",
       "      <td>0.1678</td>\n",
       "      <td>0.6577</td>\n",
       "      <td>0.70260</td>\n",
       "      <td>0.17120</td>\n",
       "      <td>0.4218</td>\n",
       "      <td>0.13410</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>14.680</td>\n",
       "      <td>20.13</td>\n",
       "      <td>94.74</td>\n",
       "      <td>684.5</td>\n",
       "      <td>0.09867</td>\n",
       "      <td>0.07200</td>\n",
       "      <td>0.07395</td>\n",
       "      <td>0.05259</td>\n",
       "      <td>0.1586</td>\n",
       "      <td>0.05922</td>\n",
       "      <td>...</td>\n",
       "      <td>30.88</td>\n",
       "      <td>123.40</td>\n",
       "      <td>1138.0</td>\n",
       "      <td>0.1464</td>\n",
       "      <td>0.1871</td>\n",
       "      <td>0.29140</td>\n",
       "      <td>0.16090</td>\n",
       "      <td>0.3029</td>\n",
       "      <td>0.08216</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>16.130</td>\n",
       "      <td>20.68</td>\n",
       "      <td>108.10</td>\n",
       "      <td>798.8</td>\n",
       "      <td>0.11700</td>\n",
       "      <td>0.20220</td>\n",
       "      <td>0.17220</td>\n",
       "      <td>0.10280</td>\n",
       "      <td>0.2164</td>\n",
       "      <td>0.07356</td>\n",
       "      <td>...</td>\n",
       "      <td>31.48</td>\n",
       "      <td>136.80</td>\n",
       "      <td>1315.0</td>\n",
       "      <td>0.1789</td>\n",
       "      <td>0.4233</td>\n",
       "      <td>0.47840</td>\n",
       "      <td>0.20730</td>\n",
       "      <td>0.3706</td>\n",
       "      <td>0.11420</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19.810</td>\n",
       "      <td>22.15</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1260.0</td>\n",
       "      <td>0.09831</td>\n",
       "      <td>0.10270</td>\n",
       "      <td>0.14790</td>\n",
       "      <td>0.09498</td>\n",
       "      <td>0.1582</td>\n",
       "      <td>0.05395</td>\n",
       "      <td>...</td>\n",
       "      <td>30.88</td>\n",
       "      <td>186.80</td>\n",
       "      <td>2398.0</td>\n",
       "      <td>0.1512</td>\n",
       "      <td>0.3150</td>\n",
       "      <td>0.53720</td>\n",
       "      <td>0.23880</td>\n",
       "      <td>0.2768</td>\n",
       "      <td>0.07615</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>13.540</td>\n",
       "      <td>14.36</td>\n",
       "      <td>87.46</td>\n",
       "      <td>566.3</td>\n",
       "      <td>0.09779</td>\n",
       "      <td>0.08129</td>\n",
       "      <td>0.06664</td>\n",
       "      <td>0.04781</td>\n",
       "      <td>0.1885</td>\n",
       "      <td>0.05766</td>\n",
       "      <td>...</td>\n",
       "      <td>19.26</td>\n",
       "      <td>99.70</td>\n",
       "      <td>711.2</td>\n",
       "      <td>0.1440</td>\n",
       "      <td>0.1773</td>\n",
       "      <td>0.23900</td>\n",
       "      <td>0.12880</td>\n",
       "      <td>0.2977</td>\n",
       "      <td>0.07259</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>13.080</td>\n",
       "      <td>15.71</td>\n",
       "      <td>85.63</td>\n",
       "      <td>520.0</td>\n",
       "      <td>0.10750</td>\n",
       "      <td>0.12700</td>\n",
       "      <td>0.04568</td>\n",
       "      <td>0.03110</td>\n",
       "      <td>0.1967</td>\n",
       "      <td>0.06811</td>\n",
       "      <td>...</td>\n",
       "      <td>20.49</td>\n",
       "      <td>96.09</td>\n",
       "      <td>630.5</td>\n",
       "      <td>0.1312</td>\n",
       "      <td>0.2776</td>\n",
       "      <td>0.18900</td>\n",
       "      <td>0.07283</td>\n",
       "      <td>0.3184</td>\n",
       "      <td>0.08183</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>9.504</td>\n",
       "      <td>12.44</td>\n",
       "      <td>60.34</td>\n",
       "      <td>273.9</td>\n",
       "      <td>0.10240</td>\n",
       "      <td>0.06492</td>\n",
       "      <td>0.02956</td>\n",
       "      <td>0.02076</td>\n",
       "      <td>0.1815</td>\n",
       "      <td>0.06905</td>\n",
       "      <td>...</td>\n",
       "      <td>15.66</td>\n",
       "      <td>65.13</td>\n",
       "      <td>314.9</td>\n",
       "      <td>0.1324</td>\n",
       "      <td>0.1148</td>\n",
       "      <td>0.08867</td>\n",
       "      <td>0.06227</td>\n",
       "      <td>0.2450</td>\n",
       "      <td>0.07773</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>15.340</td>\n",
       "      <td>14.26</td>\n",
       "      <td>102.50</td>\n",
       "      <td>704.4</td>\n",
       "      <td>0.10730</td>\n",
       "      <td>0.21350</td>\n",
       "      <td>0.20770</td>\n",
       "      <td>0.09756</td>\n",
       "      <td>0.2521</td>\n",
       "      <td>0.07032</td>\n",
       "      <td>...</td>\n",
       "      <td>19.08</td>\n",
       "      <td>125.10</td>\n",
       "      <td>980.9</td>\n",
       "      <td>0.1390</td>\n",
       "      <td>0.5954</td>\n",
       "      <td>0.63050</td>\n",
       "      <td>0.23930</td>\n",
       "      <td>0.4667</td>\n",
       "      <td>0.09946</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>21.160</td>\n",
       "      <td>23.04</td>\n",
       "      <td>137.20</td>\n",
       "      <td>1404.0</td>\n",
       "      <td>0.09428</td>\n",
       "      <td>0.10220</td>\n",
       "      <td>0.10970</td>\n",
       "      <td>0.08632</td>\n",
       "      <td>0.1769</td>\n",
       "      <td>0.05278</td>\n",
       "      <td>...</td>\n",
       "      <td>35.59</td>\n",
       "      <td>188.00</td>\n",
       "      <td>2615.0</td>\n",
       "      <td>0.1401</td>\n",
       "      <td>0.2600</td>\n",
       "      <td>0.31550</td>\n",
       "      <td>0.20090</td>\n",
       "      <td>0.2822</td>\n",
       "      <td>0.07526</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>16.650</td>\n",
       "      <td>21.38</td>\n",
       "      <td>110.00</td>\n",
       "      <td>904.6</td>\n",
       "      <td>0.11210</td>\n",
       "      <td>0.14570</td>\n",
       "      <td>0.15250</td>\n",
       "      <td>0.09170</td>\n",
       "      <td>0.1995</td>\n",
       "      <td>0.06330</td>\n",
       "      <td>...</td>\n",
       "      <td>31.56</td>\n",
       "      <td>177.00</td>\n",
       "      <td>2215.0</td>\n",
       "      <td>0.1805</td>\n",
       "      <td>0.3578</td>\n",
       "      <td>0.46950</td>\n",
       "      <td>0.20950</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.09564</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>17.140</td>\n",
       "      <td>16.40</td>\n",
       "      <td>116.00</td>\n",
       "      <td>912.7</td>\n",
       "      <td>0.11860</td>\n",
       "      <td>0.22760</td>\n",
       "      <td>0.22290</td>\n",
       "      <td>0.14010</td>\n",
       "      <td>0.3040</td>\n",
       "      <td>0.07413</td>\n",
       "      <td>...</td>\n",
       "      <td>21.40</td>\n",
       "      <td>152.40</td>\n",
       "      <td>1461.0</td>\n",
       "      <td>0.1545</td>\n",
       "      <td>0.3949</td>\n",
       "      <td>0.38530</td>\n",
       "      <td>0.25500</td>\n",
       "      <td>0.4066</td>\n",
       "      <td>0.10590</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>14.580</td>\n",
       "      <td>21.53</td>\n",
       "      <td>97.41</td>\n",
       "      <td>644.8</td>\n",
       "      <td>0.10540</td>\n",
       "      <td>0.18680</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.08783</td>\n",
       "      <td>0.2252</td>\n",
       "      <td>0.06924</td>\n",
       "      <td>...</td>\n",
       "      <td>33.21</td>\n",
       "      <td>122.40</td>\n",
       "      <td>896.9</td>\n",
       "      <td>0.1525</td>\n",
       "      <td>0.6643</td>\n",
       "      <td>0.55390</td>\n",
       "      <td>0.27010</td>\n",
       "      <td>0.4264</td>\n",
       "      <td>0.12750</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>18.610</td>\n",
       "      <td>20.25</td>\n",
       "      <td>122.10</td>\n",
       "      <td>1094.0</td>\n",
       "      <td>0.09440</td>\n",
       "      <td>0.10660</td>\n",
       "      <td>0.14900</td>\n",
       "      <td>0.07731</td>\n",
       "      <td>0.1697</td>\n",
       "      <td>0.05699</td>\n",
       "      <td>...</td>\n",
       "      <td>27.26</td>\n",
       "      <td>139.90</td>\n",
       "      <td>1403.0</td>\n",
       "      <td>0.1338</td>\n",
       "      <td>0.2117</td>\n",
       "      <td>0.34460</td>\n",
       "      <td>0.14900</td>\n",
       "      <td>0.2341</td>\n",
       "      <td>0.07421</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>15.300</td>\n",
       "      <td>25.27</td>\n",
       "      <td>102.40</td>\n",
       "      <td>732.4</td>\n",
       "      <td>0.10820</td>\n",
       "      <td>0.16970</td>\n",
       "      <td>0.16830</td>\n",
       "      <td>0.08751</td>\n",
       "      <td>0.1926</td>\n",
       "      <td>0.06540</td>\n",
       "      <td>...</td>\n",
       "      <td>36.71</td>\n",
       "      <td>149.30</td>\n",
       "      <td>1269.0</td>\n",
       "      <td>0.1641</td>\n",
       "      <td>0.6110</td>\n",
       "      <td>0.63350</td>\n",
       "      <td>0.20240</td>\n",
       "      <td>0.4027</td>\n",
       "      <td>0.09876</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>17.570</td>\n",
       "      <td>15.05</td>\n",
       "      <td>115.00</td>\n",
       "      <td>955.1</td>\n",
       "      <td>0.09847</td>\n",
       "      <td>0.11570</td>\n",
       "      <td>0.09875</td>\n",
       "      <td>0.07953</td>\n",
       "      <td>0.1739</td>\n",
       "      <td>0.06149</td>\n",
       "      <td>...</td>\n",
       "      <td>19.52</td>\n",
       "      <td>134.90</td>\n",
       "      <td>1227.0</td>\n",
       "      <td>0.1255</td>\n",
       "      <td>0.2812</td>\n",
       "      <td>0.24890</td>\n",
       "      <td>0.14560</td>\n",
       "      <td>0.2756</td>\n",
       "      <td>0.07919</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0        17.990         10.38          122.80     1001.0          0.11840   \n",
       "1        20.570         17.77          132.90     1326.0          0.08474   \n",
       "2        19.690         21.25          130.00     1203.0          0.10960   \n",
       "3        11.420         20.38           77.58      386.1          0.14250   \n",
       "4        20.290         14.34          135.10     1297.0          0.10030   \n",
       "5        12.450         15.70           82.57      477.1          0.12780   \n",
       "6        18.250         19.98          119.60     1040.0          0.09463   \n",
       "7        13.710         20.83           90.20      577.9          0.11890   \n",
       "8        13.000         21.82           87.50      519.8          0.12730   \n",
       "9        12.460         24.04           83.97      475.9          0.11860   \n",
       "10       16.020         23.24          102.70      797.8          0.08206   \n",
       "11       15.780         17.89          103.60      781.0          0.09710   \n",
       "12       19.170         24.80          132.40     1123.0          0.09740   \n",
       "13       15.850         23.95          103.70      782.7          0.08401   \n",
       "14       13.730         22.61           93.60      578.3          0.11310   \n",
       "15       14.540         27.54           96.73      658.8          0.11390   \n",
       "16       14.680         20.13           94.74      684.5          0.09867   \n",
       "17       16.130         20.68          108.10      798.8          0.11700   \n",
       "18       19.810         22.15          130.00     1260.0          0.09831   \n",
       "19       13.540         14.36           87.46      566.3          0.09779   \n",
       "20       13.080         15.71           85.63      520.0          0.10750   \n",
       "21        9.504         12.44           60.34      273.9          0.10240   \n",
       "22       15.340         14.26          102.50      704.4          0.10730   \n",
       "23       21.160         23.04          137.20     1404.0          0.09428   \n",
       "24       16.650         21.38          110.00      904.6          0.11210   \n",
       "25       17.140         16.40          116.00      912.7          0.11860   \n",
       "26       14.580         21.53           97.41      644.8          0.10540   \n",
       "27       18.610         20.25          122.10     1094.0          0.09440   \n",
       "28       15.300         25.27          102.40      732.4          0.10820   \n",
       "29       17.570         15.05          115.00      955.1          0.09847   \n",
       "\n",
       "    mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0            0.27760         0.30010              0.14710         0.2419   \n",
       "1            0.07864         0.08690              0.07017         0.1812   \n",
       "2            0.15990         0.19740              0.12790         0.2069   \n",
       "3            0.28390         0.24140              0.10520         0.2597   \n",
       "4            0.13280         0.19800              0.10430         0.1809   \n",
       "5            0.17000         0.15780              0.08089         0.2087   \n",
       "6            0.10900         0.11270              0.07400         0.1794   \n",
       "7            0.16450         0.09366              0.05985         0.2196   \n",
       "8            0.19320         0.18590              0.09353         0.2350   \n",
       "9            0.23960         0.22730              0.08543         0.2030   \n",
       "10           0.06669         0.03299              0.03323         0.1528   \n",
       "11           0.12920         0.09954              0.06606         0.1842   \n",
       "12           0.24580         0.20650              0.11180         0.2397   \n",
       "13           0.10020         0.09938              0.05364         0.1847   \n",
       "14           0.22930         0.21280              0.08025         0.2069   \n",
       "15           0.15950         0.16390              0.07364         0.2303   \n",
       "16           0.07200         0.07395              0.05259         0.1586   \n",
       "17           0.20220         0.17220              0.10280         0.2164   \n",
       "18           0.10270         0.14790              0.09498         0.1582   \n",
       "19           0.08129         0.06664              0.04781         0.1885   \n",
       "20           0.12700         0.04568              0.03110         0.1967   \n",
       "21           0.06492         0.02956              0.02076         0.1815   \n",
       "22           0.21350         0.20770              0.09756         0.2521   \n",
       "23           0.10220         0.10970              0.08632         0.1769   \n",
       "24           0.14570         0.15250              0.09170         0.1995   \n",
       "25           0.22760         0.22290              0.14010         0.3040   \n",
       "26           0.18680         0.14250              0.08783         0.2252   \n",
       "27           0.10660         0.14900              0.07731         0.1697   \n",
       "28           0.16970         0.16830              0.08751         0.1926   \n",
       "29           0.11570         0.09875              0.07953         0.1739   \n",
       "\n",
       "    mean fractal dimension   ...    worst texture  worst perimeter  \\\n",
       "0                  0.07871   ...            17.33           184.60   \n",
       "1                  0.05667   ...            23.41           158.80   \n",
       "2                  0.05999   ...            25.53           152.50   \n",
       "3                  0.09744   ...            26.50            98.87   \n",
       "4                  0.05883   ...            16.67           152.20   \n",
       "5                  0.07613   ...            23.75           103.40   \n",
       "6                  0.05742   ...            27.66           153.20   \n",
       "7                  0.07451   ...            28.14           110.60   \n",
       "8                  0.07389   ...            30.73           106.20   \n",
       "9                  0.08243   ...            40.68            97.65   \n",
       "10                 0.05697   ...            33.88           123.80   \n",
       "11                 0.06082   ...            27.28           136.50   \n",
       "12                 0.07800   ...            29.94           151.70   \n",
       "13                 0.05338   ...            27.66           112.00   \n",
       "14                 0.07682   ...            32.01           108.80   \n",
       "15                 0.07077   ...            37.13           124.10   \n",
       "16                 0.05922   ...            30.88           123.40   \n",
       "17                 0.07356   ...            31.48           136.80   \n",
       "18                 0.05395   ...            30.88           186.80   \n",
       "19                 0.05766   ...            19.26            99.70   \n",
       "20                 0.06811   ...            20.49            96.09   \n",
       "21                 0.06905   ...            15.66            65.13   \n",
       "22                 0.07032   ...            19.08           125.10   \n",
       "23                 0.05278   ...            35.59           188.00   \n",
       "24                 0.06330   ...            31.56           177.00   \n",
       "25                 0.07413   ...            21.40           152.40   \n",
       "26                 0.06924   ...            33.21           122.40   \n",
       "27                 0.05699   ...            27.26           139.90   \n",
       "28                 0.06540   ...            36.71           149.30   \n",
       "29                 0.06149   ...            19.52           134.90   \n",
       "\n",
       "    worst area  worst smoothness  worst compactness  worst concavity  \\\n",
       "0       2019.0            0.1622             0.6656          0.71190   \n",
       "1       1956.0            0.1238             0.1866          0.24160   \n",
       "2       1709.0            0.1444             0.4245          0.45040   \n",
       "3        567.7            0.2098             0.8663          0.68690   \n",
       "4       1575.0            0.1374             0.2050          0.40000   \n",
       "5        741.6            0.1791             0.5249          0.53550   \n",
       "6       1606.0            0.1442             0.2576          0.37840   \n",
       "7        897.0            0.1654             0.3682          0.26780   \n",
       "8        739.3            0.1703             0.5401          0.53900   \n",
       "9        711.4            0.1853             1.0580          1.10500   \n",
       "10      1150.0            0.1181             0.1551          0.14590   \n",
       "11      1299.0            0.1396             0.5609          0.39650   \n",
       "12      1332.0            0.1037             0.3903          0.36390   \n",
       "13       876.5            0.1131             0.1924          0.23220   \n",
       "14       697.7            0.1651             0.7725          0.69430   \n",
       "15       943.2            0.1678             0.6577          0.70260   \n",
       "16      1138.0            0.1464             0.1871          0.29140   \n",
       "17      1315.0            0.1789             0.4233          0.47840   \n",
       "18      2398.0            0.1512             0.3150          0.53720   \n",
       "19       711.2            0.1440             0.1773          0.23900   \n",
       "20       630.5            0.1312             0.2776          0.18900   \n",
       "21       314.9            0.1324             0.1148          0.08867   \n",
       "22       980.9            0.1390             0.5954          0.63050   \n",
       "23      2615.0            0.1401             0.2600          0.31550   \n",
       "24      2215.0            0.1805             0.3578          0.46950   \n",
       "25      1461.0            0.1545             0.3949          0.38530   \n",
       "26       896.9            0.1525             0.6643          0.55390   \n",
       "27      1403.0            0.1338             0.2117          0.34460   \n",
       "28      1269.0            0.1641             0.6110          0.63350   \n",
       "29      1227.0            0.1255             0.2812          0.24890   \n",
       "\n",
       "    worst concave points  worst symmetry  worst fractal dimension  target  \n",
       "0                0.26540          0.4601                  0.11890       0  \n",
       "1                0.18600          0.2750                  0.08902       0  \n",
       "2                0.24300          0.3613                  0.08758       0  \n",
       "3                0.25750          0.6638                  0.17300       0  \n",
       "4                0.16250          0.2364                  0.07678       0  \n",
       "5                0.17410          0.3985                  0.12440       0  \n",
       "6                0.19320          0.3063                  0.08368       0  \n",
       "7                0.15560          0.3196                  0.11510       0  \n",
       "8                0.20600          0.4378                  0.10720       0  \n",
       "9                0.22100          0.4366                  0.20750       0  \n",
       "10               0.09975          0.2948                  0.08452       0  \n",
       "11               0.18100          0.3792                  0.10480       0  \n",
       "12               0.17670          0.3176                  0.10230       0  \n",
       "13               0.11190          0.2809                  0.06287       0  \n",
       "14               0.22080          0.3596                  0.14310       0  \n",
       "15               0.17120          0.4218                  0.13410       0  \n",
       "16               0.16090          0.3029                  0.08216       0  \n",
       "17               0.20730          0.3706                  0.11420       0  \n",
       "18               0.23880          0.2768                  0.07615       0  \n",
       "19               0.12880          0.2977                  0.07259       1  \n",
       "20               0.07283          0.3184                  0.08183       1  \n",
       "21               0.06227          0.2450                  0.07773       1  \n",
       "22               0.23930          0.4667                  0.09946       0  \n",
       "23               0.20090          0.2822                  0.07526       0  \n",
       "24               0.20950          0.3613                  0.09564       0  \n",
       "25               0.25500          0.4066                  0.10590       0  \n",
       "26               0.27010          0.4264                  0.12750       0  \n",
       "27               0.14900          0.2341                  0.07421       0  \n",
       "28               0.20240          0.4027                  0.09876       0  \n",
       "29               0.14560          0.2756                  0.07919       0  \n",
       "\n",
       "[30 rows x 31 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Insert code here\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(cancer.data)\n",
    "#df[target]\n",
    "df.columns = cancer.feature_names\n",
    "\n",
    "\n",
    "df['target'] = cancer.target\n",
    "df.head(30)#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        t = \"(%.2f)\"%(cm[i, j])\n",
    "        #print t\n",
    "#         plt.text(j, i, t,\n",
    "#                  horizontalalignment=\"center\",\n",
    "#                  color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix, without normalization\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compute confusion matrix\n",
    "y_pred = clf.predict(cancer.data)\n",
    "cnf_matrix = confusion_matrix(cancer.target, y_pred)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=range(len(set(cancer.target))), normalize = False,\n",
    "                      title='Confusion matrix')\n",
    "\n",
    "#plt.savefig(\"confusion.png\",bbox_inches='tight')\n",
    "#plt.savefig(\"confusion.pdf\",bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the most common types of errors?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation\n",
    "**Learning the parameters of a prediction function and testing it on the same data is a methodological mistake**: a model that would just repeat the labels of the samples that it has just seen would have a perfect score but would fail to predict anything useful on yet-unseen data. This situation is called **overfitting**. To avoid it, it is common practice when performing a (supervised) machine learning experiment to hold out part of the available data as a test set `X_test`, `y_test`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn has a collection of classes which can be used to generate lists of train/test indices for popular cross-validation strategies.\n",
    "\n",
    "They expose a `split` method which accepts the input dataset to be split and yields the train/test set indices for each iteration of the chosen cross-validation strategy.\n",
    "\n",
    "Let's try with a smaller subset of the `cancer` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [ 8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31\n",
      " 32 33 34 35 36 37 38 39] | test: [0 1 2 3 4 5 6 7]\n",
      "Fold test accuracy: 100.0 %\n",
      "Train: [ 0  1  2  3  4  5  6  7 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31\n",
      " 32 33 34 35 36 37 38 39] | test: [ 8  9 10 11 12 13 14 15]\n",
      "Fold test accuracy: 87.5 %\n",
      "Train: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 24 25 26 27 28 29 30 31\n",
      " 32 33 34 35 36 37 38 39] | test: [16 17 18 19 20 21 22 23]\n",
      "Fold test accuracy: 62.5 %\n",
      "Train: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 32 33 34 35 36 37 38 39] | test: [24 25 26 27 28 29 30 31]\n",
      "Fold test accuracy: 100.0 %\n",
      "Train: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31] | test: [32 33 34 35 36 37 38 39]\n",
      "Fold test accuracy: 87.5 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "\n",
    "clf = svm.SVC(gamma=0.0001, C=100.)\n",
    "k_fold = KFold(n_splits=5)\n",
    "for train_indices, test_indices in k_fold.split(cancer.data[:40]): # consider the first 40 examples\n",
    "    print('Train: %s | test: %s' % (train_indices, test_indices))\n",
    "    clf.fit(cancer.data[train_indices], cancer.target[train_indices])\n",
    "    print('Fold test accuracy: {} %'.format(clf.score(cancer.data[test_indices], cancer.target[test_indices])*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now try with the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy: 94.19486215538848 %\n"
     ]
    }
   ],
   "source": [
    "score = []\n",
    "clf = svm.SVC(gamma=0.0001, C=100.)\n",
    "k_fold = KFold(n_splits=10)\n",
    "for train_indices, test_indices in k_fold.split(cancer.data):\n",
    "    clf.fit(cancer.data[train_indices], cancer.target[train_indices])\n",
    "    score.append(clf.score(cancer.data[test_indices], cancer.target[test_indices]))\n",
    "print('Average accuracy: {} %'.format(np.mean(score)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a shortcut, we can use `cross_val_score` for the same purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy: 94.19486215538848 %\n"
     ]
    }
   ],
   "source": [
    "score_2 = cross_val_score(clf, cancer.data, cancer.target, cv=k_fold, n_jobs=-1)\n",
    "print('Average accuracy: {} %'.format(np.mean(score_2)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid-search\n",
    "\n",
    "Scikit-learn provides an object that, given data, computes the score during the fit of an estimator on a parameter grid and chooses the parameters to maximize the cross-validation score. This object takes an estimator during the construction and exposes an estimator API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best C: 1000.0 Best gamma: 1e-06 Fold test accuracy: 0.8859649122807017\n",
      "Best C: 1000.0 Best gamma: 1e-06 Fold test accuracy: 0.9649122807017544\n",
      "Best C: 1000.0 Best gamma: 1e-05 Fold test accuracy: 0.9736842105263158\n",
      "Best C: 1000.0 Best gamma: 1e-05 Fold test accuracy: 0.9736842105263158\n",
      "Best C: 1000.0 Best gamma: 1e-05 Fold test accuracy: 0.9380530973451328\n",
      "Average accuracy: 94.72597422760441 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "clf = svm.SVC(gamma=0.01, C=10.)\n",
    "Cs = np.logspace(-1, 3, 9)\n",
    "Gs = np.logspace(-7, -0, 8)\n",
    "clf = GridSearchCV(estimator=clf, param_grid=dict(C=Cs, gamma=Gs), n_jobs=-1)\n",
    "\n",
    "score = []\n",
    "k_fold = KFold(n_splits=5)\n",
    "for train_indices, test_indices in k_fold.split(cancer.data):\n",
    "    clf.fit(cancer.data[train_indices], cancer.target[train_indices])\n",
    "    score.append(clf.score(cancer.data[test_indices], cancer.target[test_indices]))\n",
    "    print('Best C:', clf.best_estimator_.C,\n",
    "          'Best gamma:', clf.best_estimator_.gamma,\n",
    "          'Fold test accuracy:', score[-1])\n",
    "print('Average accuracy: {} %'.format(np.mean(score)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'log(gamma)')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Grid results for last fold\n",
    "scores = clf.cv_results_['mean_test_score'].reshape(len(Cs), len(Gs))\n",
    "extent = np.log10([Gs[0], Gs[-1], Cs[0], Cs[-1]])\n",
    "\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "im = plt.imshow(scores, extent=extent, origin='lower')\n",
    "plt.colorbar(im)\n",
    "plt.contour(np.log10(Gs), np.log10(Cs), scores)\n",
    "plt.xlabel('log(C)')\n",
    "plt.ylabel('log(gamma)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now try to use a different classifier. For example, we will now try a Decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "k_fold = KFold(n_splits=10)\n",
    "#clf = tree.DecisionTreeClassifier(criterion = \"entropy\")\n",
    "clf = RandomForestClassifier(n_estimators = 2000,max_depth = 4)\n",
    "score_tree = cross_val_score(clf, cancer.data, cancer.target, cv=k_fold, n_jobs=-1)\n",
    "print('Average accuracy:', np.mean(score_tree))\n",
    "\n",
    "# Now fit the tree\n",
    " # TODO: insert code here\n",
    "clf.fi( cancer.data, cancer.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now check the new confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: insert code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at what are the most important features from our dataset according to the decision tree classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = clf.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in clf.estimators_], axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "print(indices)\n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "for f in range(n_features):\n",
    "    print(\"%d. %s (%f)\" % (f + 1, cancer.feature_names[indices[f]],  importances[indices[f]]))\n",
    "\n",
    "# Plot the feature importances of the forest\n",
    "fig = plt.figure()\n",
    "plt.title(\"Feature importances\")  # just the top 10 features\n",
    "num_feat_to_plot = 10\n",
    "plt.bar(range(num_feat_to_plot), importances[indices[:num_feat_to_plot]],\n",
    "       color=\"r\", yerr=std[indices[:num_feat_to_plot]], align=\"center\")\n",
    "plt.xticks(range(num_feat_to_plot), np.array(cancer.feature_names)[indices[:num_feat_to_plot]])\n",
    "plt.xlim([-1, num_feat_to_plot])\n",
    "fig.set_size_inches(15,8)\n",
    "axes = plt.gca()\n",
    "axes.set_ylim([0,None])\n",
    "\n",
    "#plt.savefig(\"importances.png\",bbox_inches='tight')\n",
    "#plt.savefig(\"importances.pdf\",bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now plot the distributions of values of the top 5 features. Are there any relationships between them? You can use functions from seaborn such as distplot or jointplot to look at this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standardisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did you figure out whether your data should be standardised? If so, play with different methods from the sklearn library.\n",
    "You can get some ideas from here: https://scikit-learn.org/stable/modules/preprocessing.html\n",
    "\n",
    "Make sure your data still looks OK before attempting to re-train a classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you've standardised your data (if required), try to fit an SVM classifier again. Is the performance affected by this processing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
